from sklearn.metrics import mean_absolute_error, mean_squared_error
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
import numpy as np
import joblib


def train_lstm(X_train, y_train, X_test, y_test, scaler, epochs=50, batch_size=32):
    model = Sequential()
    model.add(LSTM(64, return_sequences=False, input_shape=(X_train.shape[1], X_train.shape[2])))
    model.add(Dense(1))

    model.compile(optimizer='adam', loss='mse')
    history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,
                        validation_data=(X_test, y_test), verbose=1)

    # 쮏햫쮏
    y_pred = model.predict(X_test)

    # 뉋햆햏뤯뉋햇햏햏햞 먫돯냻햛햏햊햔 행햏햃햖햇햏햛 CLOSE (혢햣햣향 scaler.inverse_transform)
    # reshape 햢 (samples, 1), 혤쮏 햪쮏웷쫧 햠혞햩 쮏'혮햢햫햟혝햦 햫햟향햟햢
    y_pred_full = np.concatenate([y_pred, np.zeros((len(y_pred), 2))], axis=1)
    y_test_full = np.concatenate([y_test.reshape(-1, 1), np.zeros((len(y_test), 2))], axis=1)

    y_pred_real = scaler.inverse_transform(y_pred_full)[:, 0]
    y_test_real = scaler.inverse_transform(y_test_full)[:, 0]

    # 햎햣혝햦햨햦
    mae = mean_absolute_error(y_test_real, y_pred_real)
    rmse = mean_squared_error(y_test_real, y_pred_real)
    print(f"\n游늵 MAE: {mae:.5f}, RMSE: {rmse:.5f}")

    model.save("model.keras")
    joblib.dump(scaler, "scaler.save")

    return model, history, y_pred_real, y_test_real
