from sklearn.metrics import mean_absolute_error, mean_squared_error
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.optimizers import Adam
import numpy as np
import joblib


def train_lstm(X_train, y_train, X_test, y_test, scaler, epochs=20, batch_size=16):

    model = Sequential()
    model.add(LSTM(units=128, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))
    model.add(LSTM(units=64, return_sequences=False))
    model.add(Dropout(0.2))  # 쮏혡혰쮏쫧썛혧햫
    model.add(Dense(units=1))  # 쮏햫쮏 쮏얧쫧쮏 향햫햟혢햣햫햫혪 (햫햟. 혡혰햫햟)

    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])

    history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,
                        validation_data=(X_test, y_test), verbose=1)

    # 쮏햫쮏
    y_pred = model.predict(X_test)

    # 뉋햆햏뤯뉋햇햏햏햞 먫돯냻햛햏햊햔 행햏햃햖햇햏햛 CLOSE (혢햣햣향 scaler.inverse_transform)
    # reshape 햢 (samples, 1), 혤쮏 햪쮏웷쫧 햠혞햩 쮏'혮햢햫햟혝햦 햫햟향햟햢
    y_pred_full = np.concatenate([y_pred, np.zeros((len(y_pred), 2))], axis=1)
    y_test_full = np.concatenate([y_test.reshape(-1, 1), np.zeros((len(y_test), 2))], axis=1)

    y_pred_real = scaler.inverse_transform(y_pred_full)[:, 0]
    y_test_real = scaler.inverse_transform(y_test_full)[:, 0]

    # 햎햣혝햦햨햦
    mae = mean_absolute_error(y_test_real, y_pred_real)

    print(f"\n游늵 MAE: {mae:.5f}")

    model.save("model.keras")
    joblib.dump(scaler, "scaler.save")

    return model, history, y_pred_real, y_test_real
